\section{Variational Inference}
When computing the posterior density of parameters according to observed data, we may want to simplify the computation by approximating the posterior density by a simpler density that does not involve the observed data. One way to do so is the variational inference, which gives an approximation of the posterior distribution as a result of an optimization problem that minimizes an measure of "closeness".\\
\newline
We suppose we have observations $x$ and parameters $z$, we are looking to approximate the posterior conditional distribution $p(z|x)$. Given a family of densities $\mathcal{D}$ over the parameters, we want to find the distribution $q \in \mathcal{D}$ that is the closest possible to our target distribution $p(z|x)$.\\
\newline
The most prominent divergence measure used in statistics is the Kullback-Leibler (KL) divergence:
\begin{equation}
KL(p||q) := \int q(\theta)\log\left(\frac{q(\theta)}{p(\theta|y)}\right) d\theta
\label{eq:KL_div}
\end{equation}
It has been introduced by Kulback and Leibler, who described it as a "directed divergence" as it is asymmetric, \textit{i.e. }$KL(p||q) \neq KL(q||p)$.\\
\newline
We try to optimize the family of densities over latent variables, parametrized by variational parameters. Finding the best suitable family is finding the best settings of parameters closer in $KL$ to the desired distribution. We are looking for $\mathcal{D}$ flexible enough for the approximation $q \in \mathcal{D}$ to be close $p(z|x)$ w.r.t. the $KL$ divergence but simple enough for efficient optimization.\\

\begin{equation}
q^*(z) = \arg\min_{q(z) \in \mathcal{D}} KL(q(z)||p(z|x))
\label{eq:best_q}
\end{equation}

\subsection{Evidence Lower Bound}

Assume $\mathcal{D}$ a density family, $q(z) \in \mathcal{D}$ a candidate approximation for $p(z|x)$.
\begin{align}
KL(q(z)||p(z|x)) &= \mathbb{E}\left[\log q(z)\right] - \mathbb{E}\left[\log p(z|x)\right]\\
&= \mathbb{E}\left[\log q(z)\right] - \mathbb{E}\left[\log p(z,x)\right] + \log p(x)
\label{eq:KL_to_logP}
\end{align} 
We call the evidence lower bound (ELBO) :
\begin{equation}
ELBO(q) = \mathbb{E}\left[\log p(z,x)\right] - \mathbb{E}\left[\log q(z)\right]
\label{eq:ELBO}
\end{equation}
We have :
\begin{equation}
\log p(x) = \underbrace{KL(q||p)}_{\geq 0} + ELBO(q) \Rightarrow \log p(x) \geq ELBO(q)
\label{eq:ELBO+KL}
\end{equation}
Hence, minimizing the KL divergence is equivalent to maximizing the ELBO.\\

