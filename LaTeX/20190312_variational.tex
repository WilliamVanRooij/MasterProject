\section{Variational Inference}
When working with Bayesian variable selection, one usually wants to compute the posterior marginal distributions, \textit{i.e.} the inference. To approximate some complicated to compute densities, one of the options is to use variational inference. It is a way to approximate by a more tractable distribution the conditional density of latent variables given observed variables.\\
We suppose we have $n$ observations $x$ and $m$ parameters $z$, we are looking for the conditional distribution $p(z|x)$. Given a family of densities $\mathcal{D}$ over the parameters, we want to find the distribution $q \in \mathcal{D}$ that minimizes the Kullback-Leiber divergence. \\

\begin{equation}
KL(p||q) := \int q(\theta)\log\left(\frac{q(\theta)}{p(\theta|y)}\right) d\theta
\label{eq:KL_div}
\end{equation}

We try to optimize the family of densities over latent variables, parametrized by variational parameters. Finding the best suitable family is finding the best settings of parameters closer in $KL$ to the desired distribution. We are looking for $\mathcal{D}$ flexible enough for the approximation $q \in \mathcal{D}$ to be close $p(z|x)$ w.r.t. the $KL$ divergence but simple enough for efficient optimization.\\

\begin{equation}
q^*(z) = \arg\min_{q(z) \in \mathcal{D}} KL(q(z)||p(z|x))
\label{eq:best_q}
\end{equation}

\subsection{Evidence Lower Bound}

Assume $\mathcal{D}$ a density family, $q(z) \in \mathcal{D}$ a candidate approximation for $p(z|x)$.
\begin{align}
KL(q(z)||p(z|x)) &= \mathbb{E}\left[\log q(z)\right] - \mathbb{E}\left[\log p(z|x)\right]\\
&= \mathbb{E}\left[\log q(z)\right] - \mathbb{E}\left[\log p(z,x)\right] + \log p(x)
\label{eq:KL_to_logP}
\end{align} 
We define the evidence lower bound (ELBO) :
\begin{equation}
ELBO(q) = \mathbb{E}\left[\log p(z,x)\right] - \mathbb{E}\left[\log q\right]
\label{eq:ELBO}
\end{equation}
We have :
\begin{equation}
\log p(x) = \underbrace{KL(q||p)}_{\geq 0} + ELBO(q) \Rightarrow \log p(x) \geq ELBO(q)
\label{eq:ELBO+KL}
\end{equation}
Hence, minimizing the KL divergence is equivalent to maximizing the ELBO.\\

