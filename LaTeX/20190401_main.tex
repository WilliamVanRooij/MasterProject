\documentclass{article}
\usepackage[british]{babel}
\usepackage{times}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{verbatim}
\usepackage{lmodern}
\usepackage{tikz}
\usepackage[ruled,vlined]{algorithm2e}
\usetikzlibrary{patterns}

\numberwithin{equation}{section}

\newtheorem{lemma}{Lemma}[section]

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator*{\KL}{{\rm KL}}

\setlength{\parskip}{1em}

\begin{document}

\title{Statistical analysis on genomic data}
\author{William van Rooij - EPFL}

\maketitle

\newpage
\tableofcontents
\newpage
\section{Introduction}
For the past years, data science has been increasingly present in the world. From financial establishments to road management companies, a lot of industry sectors are integrating data science in the way business is done. With the expansion of computer performances, we are able to compute calculations quicker and can work with more complex models. The volume of collectible data, hence analysable data, is also growing, which allows more and more parameters to be included in our analysis.

Usually, when trying to find a model fitting the observable data, we have much more observations than parameters to fit. We work in a \textit{large n - small p} situation. It is the most common type of statistical analysis, Bayesian hierarchy modelling is a strong tool to identify the dependencies across multiple sources of informations.

However, in some cases, the number of parameters we need to estimate is way larger than the number of observations available. It is often the case in genomic research, where the situation is called a \textit{small n - large p}. Traditional techniques do not apply in these environment, because of the computation time necessary for the algorithms to end, or the accuracy not adequate enough.

In this report, we will focus on the \textit{small n - large p} situation in the context of genetic association. We will focus on high dimensional Bayesian inference, with its statistical advantages and its computational problem that dissuades users to adopt this solution in statistical applications.

\subsection{Motivation}
The state of the technology nowadays allows us to determine the human genome. With this possibility, a whole new set of data is available to study the association between these data and various diseases or phenotypes.  Part of these newly available data are \textit{genetic variants}, a change at a specific location on  the genome (locus), where the different versions are called \textit{alleles}. We will focus on the most common genetic variant, the \textit{single nucleotide polymorphisms} (SNPs), a variation in the nucleotide that is present to some appreciable extent in the population, \textit{i.e.} the \textit{minor} allele has frequency $> 0.01$ \cite{lewin}. Some combinations of SNPs are inherited together, which yields block-wise dependences structures. We observe strong autocorrelations in these blocks, as shows Figure \ref{fig:corr}.
\begin{figure}
\centering
\includegraphics[width=3in,bb = 0 0 2in 2in]{images/corrWeights.png}
\caption{\label{fig:corr} Visualisation of the block-wise correlation structure and the probabilities of association of SNPs to a phenotype.}
\end{figure}

We focus on \textit{expression quantitative trait locus} (eQTL) analyses, which study the effects of genetic variants, in our case SNPs, on the expression of transcripts, or genes. The data used for eQTL studies consists generally of several hundreds thousands SNPs and thousand transcripts expression outcomes. It is, in fact, a \textit{small n - large p - large q} situation, where $p$ is the number of SNPs and $q$ is the number of transcripts expressions outcomes.

We would like to represent these data by a model where $\boldsymbol{y} = (y_1, \ldots ,y_q)$, the transcripts expressions, are linearly related with the predictors $\boldsymbol{X} = (X_1, \ldots, X_p)$, the SNPs, and $\boldsymbol{\beta}$ represent the probabilities of associations between the SNPs and the transcript expressions, \textit{i.e.}:
\begin{equation*}
\boldsymbol{y}_{n\times q} = \boldsymbol{x}_{n \times p}\;\boldsymbol{\beta}_{p \times q}+\boldsymbol{\epsilon}_{n \times q},\;\boldsymbol{\epsilon}_t \sim \mathcal{N}(0,\tau_t^{-1}I_n),
\end{equation*}
where $\tau_t$ is the residual precision.

In a Bayesian framework, one of the main tools used are integrals. However, the integrals are usually not amenable and need to be approximated. Markov Chain Monte Carlo (MCMC) algorithms are the most used to estimate these integrals and are fairly quick and accurate when working on reasonably small datasets. When the dataset dimensions grow, however, the MCMC algorithms become really time-consuming up to not being computable.

When performing MCMC inference, likelihoods and sometimes gradients need to be calculated at each iterations. The cost of these calculations increases with the number of parameters. Moreover, the more dimensions the problem has, the less accurate the approximations become, which leads to more iterations to keep the precision needed. For the algorithm to end, all the parameters need to have converged, which means all parameters need to be checked and stored, which is close to impossible when their number is really high.

In our situation, \textit{small n - large p - large q}, the computational cost of using an MCMC algorithm is huge. The time and memory needed to perform the algorithm is not acceptable as of today. We have to use an alternative solution, which is called variational inference \cite{varInf}.
%==========================================
\newpage
\section{Model}
We denote $\boldsymbol{X }= (X_1,\ldots,X_p)$ the SNPs and $\boldsymbol{y} = (y_1,\ldots,y_q)$ the traits. The SNPs are strongly correlated. Our goal is to estimate the association between SNP $s$ and transcripts expression , called \textit{trait}, $t$. To do so, we consider $\boldsymbol{y}$ as the response matrix and $\boldsymbol{X}$ as the candidate predictors of the linear model where each response $y_t$ is linearly related with the predictors $\boldsymbol{X}$ and has a residual precision $\boldsymbol{\tau}_t\sim$ Gamma$(\eta_t,\kappa_t)$, \textit{i.e.}:
\begin{equation*}
\label{eq:model}
\boldsymbol{y}_{n\times q} = \boldsymbol{X}_{n \times p}\;\boldsymbol{\beta}_{p \times q}+\boldsymbol{\epsilon}_{n \times q}\text{, }\boldsymbol{\epsilon}_t \sim \mathcal{N}(0,\tau_t^{-1}I_n)
\end{equation*}
where $\boldsymbol{\beta}_{st}$ represent the association between SNP $s$ and trait $t$

We introduce $\boldsymbol{\gamma}_{p\times q}$, a binary matrix corresponding to the association of SNPs and traits. If the SNP $s$ and trait $t$ are associated, then $\gamma_{st} = 1$. To enforce sparsity in $\boldsymbol{\beta}$, we place a "spike-and-slab" prior distribution:
\begin{equation*}
\beta_{st} \mid \gamma_{st},\sigma^2, \tau_t \sim \gamma_{st}\;\mathcal{N}(0,\sigma^2\tau_t^{-1})+(1-\gamma_{st})\delta_0,\;t=1,\ldots,q
\end{equation*}
 
% =========================================
\newpage
\section{Variational inference}
When computing the posterior density of parameters $\boldsymbol{\theta}$ according to observed data $\boldsymbol{y}$, variational inference simplifies the computation by approximating the posterior density $p(\boldsymbol{\theta}\mid \boldsymbol{y})$ with a simpler density $q(\boldsymbol{\theta})$. It gives an approximation of the posterior distribution as a result of an optimization problem that minimizes a measure of "closeness" as objective function.

We suppose we have observations $y$ and parameters $\boldsymbol{\theta}$, we are looking to determine the posterior distribution of the parameters conditional on the observations $p(\boldsymbol{\theta} \mid \boldsymbol{y})$. Given a family of densities $\mathcal{D}$ over the parameters, we want to find the distribution $q \in \mathcal{D}$ that minimizes the "closeness" measure compared to $p(\boldsymbol{\theta} \mid \boldsymbol{y})$.

Variational inference minimizes the Kullback--Leibler divergence as a "closeness" measure. Introduced in 1951 by Kullback and Leibler\cite{kl51}, it is the most common divergence measure used in statistics and machine learning. It is described as such:
\begin{equation*}
\KL(q\parallel p) := \int q(\boldsymbol{\theta})\log \left(\frac{q(\boldsymbol{\theta})}{p(\boldsymbol{\theta} \mid \boldsymbol{y})}\right) \mathrm{d}\boldsymbol{\theta}.
\end{equation*} 
It is described as a "directed divergence" as it is asymmetric, \textit{i.e.} $\KL(p\parallel q) \neq \KL(q \parallel p)$.

Determining the family $\mathcal{D}$ can be difficult as we need the family to be simple enough to be optimized efficiently, but flexible enough for the approximation $q \in \mathcal{D}$ to be close to $p(\boldsymbol{\theta} \mid \boldsymbol{y})$ with respect to the Kullbach--Leibler divergence. The approximation will then be:
\begin{equation*}
q^*(\boldsymbol{\theta} ) = \argmin_{q(\boldsymbol{\theta}) \in \mathcal{D}} \KL\left( q(\boldsymbol{\theta}) \parallel p(\boldsymbol{\theta} \mid \boldsymbol{y})\right).
\end{equation*}

Minimizing the Kullbach--Leibler divergence can be complicated depending on the density $p$ that we want to approximate and the densities family $\mathcal{D}$ we want $q$ to be part of. We can decompose the KL divergence as follows:
\begin{align*}
\KL\left(q(\boldsymbol{\theta})||p(\boldsymbol{\theta}\mid \boldsymbol{y})\right) &= \mathbb{E}\left[\log q(\boldsymbol{\theta})\right] - \mathbb{E}\left[\log p(\boldsymbol{\theta}\mid \boldsymbol{y})\right]\\
&= \mathbb{E}\left[\log q(\boldsymbol{\theta})\right] - \mathbb{E}\left[\log p(\boldsymbol{y},\boldsymbol{\theta})\right] + \log p(\boldsymbol{y}).
\end{align*}
We introduce the evidence lower bound (\small{ELBO}):
\begin{align*}
\mathcal{L}(q) &= \mathbb{E}\left[\log p(\boldsymbol{\theta},\boldsymbol{y})\right] - \mathbb{E}\left[\log q(\boldsymbol{\theta})\right]\\
&=\int q(\boldsymbol{\theta})\log\frac{p(\boldsymbol{y},\boldsymbol{\theta})}{q(\boldsymbol{\theta})}\mathrm{d}\boldsymbol{\theta}.
\end{align*}
When decomposing the KL divergence, we obtain:
\begin{equation*}
\KL(q\parallel p) = \log(p) - \mathcal{L}(q).
\end{equation*}

This means that the KL divergence is the difference between the marginal log-likelihood with no effect on the optimization and a function : $\mathcal{L}(q)$. So minimizing the Kullbach--Leibler divergence is the same as maximizing $\mathcal{L}(q)$. The difference lays in the complexity of the problems, minimizing the Kullbach--Leibler divergence is not tractable, but maximizing $\mathcal{L}(q)$ admits a closed form when the family of densities $\mathcal{D}$ is well chosen. In such a case, we prefer to use $\mathcal{L}(q)$ as an objective function.

Using Jensen's inequality, we can show that $\mathcal{L}(q)$ is a lower bound for the marginal log-likelihood, which is why we call it the evidence lower bound, or variational lower bound.
\begin{align*}
\log p(\boldsymbol{y}) &= \log \int p(\boldsymbol{y}, \boldsymbol{\theta}) \mathrm{d}\boldsymbol{\theta},\\
&= \log \int \frac{p(\boldsymbol{y}, \boldsymbol{\theta})}{q(\boldsymbol{\theta})}q(\boldsymbol{\theta})\mathrm{d}\boldsymbol{\theta}
,\\
&\geq \int q(\boldsymbol{\theta}) \log \left\lbrace \frac{p(\boldsymbol{y}, \boldsymbol{\theta})}{q(\boldsymbol{\theta})} \right\rbrace \mathrm{d}\boldsymbol{\theta},\\
&= \mathcal{L}(q).
\end{align*}
Hence, $\log p(\boldsymbol{y}) \geq \mathcal{L}(q)$, justifying the name "lower bound" for $\mathcal{L}(q)$.

\subsection{Mean-field approximation}
The complexity of the optimization problem is directly bound to the complexity of the family of densities $\mathcal{D}$ we want $q(\boldsymbol{\theta})$ to be apart of. We introduce the mean-field variational family, where the parameters are mutually independent and are governed by a distinct factor in the variational density.

We introduce $\left\lbrace \theta_j\right\rbrace_{j=1}^J$ a partition of $\boldsymbol{\theta}$, if $q \in \mathcal{D}$ and $\mathcal{D}$ a mean-field variational family, then:
\begin{equation*}
q(\boldsymbol{\theta}) = \prod_{j=1}^J q_j(\theta_j)
\end{equation*}
We determine the variational factors $q_j(\theta_j)$ by maximizing $\mathcal{L}(q_j)$. Hence, the variational family does not directly represent the observed data, they are both linked through the optimization of the \small{ELBO}.

To visualise the mean-field approximation, we consider a two dimensional Gaussian distribution, represented in clear in Figure \ref{fig:mean_field}. The mean-field approximation of the posterior distribution is represented by the barred circle. We can see that the mean of the approximation is the same as the real mean, but the covariance doesn't match the covariance of the real posterior.

\begin{figure}[h!]
\centering
\begin{tikzpicture}
\draw[thick, ->] (0,-2) -- (0,2);
\draw[thick, ->] (-2,0) -- (2,0);
\fill[pattern=north west lines,opacity=.6,draw] (0,0) circle (1cm);
\draw[rotate=-45] (0,0) ellipse (0.65cm and 2cm);
\node (p) at (2,1.7) {Real posterior};
\node (q) at (2.7,-0.9) {Mean-field approximation};
\node (x1) at (-0.3,2) {$x_1$};
\node (x2) at (2,-0.3) {$x_2$};
\end{tikzpicture}
\caption{\label{fig:mean_field}Visualisation of mean-field approximation to a two-dimensional Gaussian posterior. The correlations in the mean-field approximation do not represent the correlations of the real posterior.}
\end{figure}

We have transformed, using the \small{ELBO} and the mean-field approximation our problem into a optimization problem. We now need a way to solve this problem. In the following section, we will be looking at the coordinate ascend mean-field variational inference.
% =========================================
\subsection{Coordinate ascent algorithm}
The coordinate ascent mean-field variational inference (\small{CAVI}) is one of the most common used to solve this kind of optimization problem. The algorithm iterates on the parameters of the mean-field approximation, optimizing them one at the time. It yields a local optimum for the \small{ELBO}.

The \small{CAVI} algorithm is based on the following result:
\begin{lemma}

If we fix $q_l(\theta_l)$, $l\neq j$, then the optimal $q^*_j(\theta_j)$ verifies:
\begin{equation*}
q^*_j(\theta_j) \propto \exp\left\lbrace\mathbb{E}_{-j}\left[\log p(\theta_j \mid \boldsymbol{\theta}_{-j}, \boldsymbol{y})\right]\right\rbrace.
\end{equation*}

\end{lemma}

Based on this result, it updates one parameter $\theta_j$ at the time while the others are fixed. The CAVI algorithm stops when the \small{ELBO} varies less than a determined threshold $\varepsilon$.\\
\newline
\begin{algorithm}
\SetKwData{ELBO}{$\mathcal{L}(q)$}\SetKwData{OLDELBO}{$\mathcal{L}^{\text{old}}(q)$}
\SetKwFunction{Union}{Union}\SetKwFunction{FindCompress}{FindCompress}
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}\SetKwInOut{Init}{initialize}
\SetKw{Set}{set}
\Input{$p(\boldsymbol{y},\boldsymbol{\theta})$, dataset $y$ tolerance $\varepsilon$}
\BlankLine
\Output{$q(\boldsymbol{\theta}) = \prod_{j=1}^J q_j(\theta_j)$}
\BlankLine
\Init{$q_j(\theta_j)$}
\BlankLine
\Repeat{$|\mathcal{L}^{\text{old}}(q)-\mathcal{L}(q)| < \varepsilon$}{
\For{$j\in \left\lbrace 1, \ldots, J \right\rbrace $}{\label{forins}
\Set{$q_j(\theta_j) \propto \exp\left\lbrace\mathbb{E}_{-j}\left[\log p(\theta_j \mid \boldsymbol{\theta}_{-j}, \boldsymbol{y})\right]\right\rbrace$}}
\BlankLine
\OLDELBO$\leftarrow$\ELBO\;
\ELBO$\leftarrow\mathbb{E}\left[\log p(\boldsymbol{\theta}, \boldsymbol{y})\right]-\mathbb{E}\left[\log q(\boldsymbol{\theta})\right]$
\BlankLine
}
\Return{$q(\boldsymbol{\theta})$}
\caption{Coordinate ascent variational inference (\small{CAVI})\label{algo:CAVI}}
\end{algorithm}

At every iteration, $\mathcal{L}(q)$ is guaranteed to increase. The CAVI yields a local optimum depending on the initialization of the $q_j(\theta_j)$, $j=1,\ldots,J$. Having different initializations could yield different optimums. These different optimums correspond to different sets of parameters $\boldsymbol{\theta}_k$, $k=1,\ldots,K$

\section{Multimodality}

Bayesian model averaging is a solution to the inference problem with multiple competing models. It consists of weighting the different models in a weighted average with the probability that the data corresponds to such a model. If multiple models are strongly probable of corresponding to the data, it can be seen in the result of the Bayesian model averaging. The more the model corresponds to the observed data, the more it will stand out in the result.

Assume the data $\boldsymbol{y}$ could correspond to multiple models $M_k$, $k= 1,\ldots,K$, and $\Delta$ is the quantity of interest. We have the posterior distribution:
\begin{equation}
p(\Delta \mid \boldsymbol{y}) = \sum_{k=1}^K p(\Delta \mid M_k,\boldsymbol{y}) \; p(M_k \mid \boldsymbol{y}).
\label{eq:post_dist}
\end{equation}

This corresponds to a weighted average of the posterior distribution under each of the considered models with weights corresponding to the posterior models probabilities. The posterior probability for model $M_k$ is given by:
\begin{equation}
p(M_k \mid \boldsymbol{y}) = \frac{p(\boldsymbol{y} \mid M_k)\; p(M_k)}{\sum_{j=1}^K p(\boldsymbol{y} \mid M_j)\; p(M_j)},
\label{eq:post_prob}
\end{equation}
where $p(\boldsymbol{y} \mid M_k)$ is the likelihood of model $M_k$, and $p(M_k)$ is the prior probabilities of the model $M_k$. It can, for example, depend on the complexity of the model, to favour the simpler models, or, if we consider the models to be equiprobable, it would be equal to $p(M_k) = 1/K$, $k = 1,\ldots,K$.  

We know that the \small{ELBO} and the KL divergence are related and that minimizing the KL divergence is equivalent to maximizing the \small{ELBO}, and that they verify: 
\begin{equation*}
\KL(q\parallel p) = \log p (\boldsymbol{y}) - \mathcal{L}(q).
\end{equation*}
As we minimized the KL div, we can use $\mathcal{L}(q)$ as an approximation for $\log p(\boldsymbol{y})$ in Equation \ref{eq:post_prob}.

Now, instead of $p(\Delta \mid \boldsymbol{y})$ in Equation \ref{eq:post_dist}, we might be interested in approximating:
\begin{equation*}
\mathbb{E}\left[\Delta \mid \boldsymbol{y}\right] = \sum_{k=1}^K\mathbb{E}\left[\Delta \mid M_k, \boldsymbol{y}\right]\;p(M_k \mid \boldsymbol{y}).
\end{equation*}
The same way we did in Equation \ref{eq:post_dist}, we calculate $p(M_K \mid \boldsymbol{y})$ with Equation \ref{eq:post_prob}.

% =========================================

%\newpage
%\section{Results}
%\begin{figure}
%\includegraphics[width=5in]{images/multipleProba1.jpg}
%\includegraphics[width=5in]{images/multipleProba2.jpg}
%\end{figure}
%
%\begin{figure}
%\includegraphics[width=5in]{images/AUC_100_iter.jpeg}
%\end{figure}
%
%\begin{figure}
%\includegraphics[width=2.7in]{images/ROC_Comp_p0_15_var_0_5.jpeg}
%\includegraphics[width=2.7in]{images/ROC_Comp_p0_15_var_0_8.jpeg}
%\includegraphics[width=2.7in]{images/ROC_Comp_p0_50_var_0_5.jpeg}
%\includegraphics[width=2.7in]{images/ROC_Comp_p0_50_var_0_8.jpeg}
%\caption{Top: $p_0 = 15$, Left: Max tot. PVE$ = 0.5$,
%Bottom: $p_0 = 50$, Right: Max tot. PVE$ = 0.8$}
%\end{figure}
% =========================================
\newpage
\section{Next steps}
In the continuity of this project, we would like to optimize the code of the function we implemented. To have an accepatable comparison with the other methods commonly used. If the results are satisfactory, we may include the function in H. Ruffieux's R-package.

We would like to compare the accuracy and computation cost to other methods, such as annealing and non-weighted averaging for strong correlations. We would also like to try to combine annealing with our method to see the way it would interact.

To be able to tell if the right modes are obtained from the ascent algorithm, we would like to represent with the level curves the modes and the direction of the algorithm, the same way V. Rockova did.

Finally, to be able to apply this method on real-life data would be the target of the whole project. To be sure that the method works not only on simulated data, but could be used properly.
\newpage
\bibliography{references}
\bibliographystyle{plain}
\end{document}